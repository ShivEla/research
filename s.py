# -*- coding: utf-8 -*-
"""nlp_finallab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O81pIXPsbdcMmPXjvbbVV-mLk8Bb5xmj

## CYK and P-CYK parser
"""

from collections import defaultdict
from nltk import Tree
import nltk

# Ensure nltk resources are available
nltk.download('punkt')

# ---------------------------
# CYK PARSER (Non-Probabilistic)
# ---------------------------
def cyk_parser(grammar, tokens):
    n = len(tokens)
    table = [[set() for _ in range(n)] for _ in range(n)]
    back = [[defaultdict(list) for _ in range(n)] for _ in range(n)]

    # Inverse grammar mapping: RHS -> LHS
    rhs_to_lhs = defaultdict(set)
    for lhs, rules in grammar.items():
        for rhs in rules:
            rhs_to_lhs[tuple(rhs)].add(lhs)

    # Fill diagonals
    for i, token in enumerate(tokens):
        for lhs in rhs_to_lhs.get((token,), []):
            table[i][i].add(lhs)

    # Fill upper triangle
    for l in range(2, n+1):
        for i in range(n - l + 1):
            j = i + l - 1
            for k in range(i, j):
                for B in table[i][k]:
                    for C in table[k+1][j]:
                        for A in rhs_to_lhs.get((B, C), []):
                            table[i][j].add(A)
                            back[i][j][A].append((k, B, C))

    # Recursive tree building
    def build_tree(i, j, symbol):
        if i == j:
            return (symbol, tokens[i])
        for k, B, C in back[i][j].get(symbol, []):
            left = build_tree(i, k, B)
            right = build_tree(k+1, j, C)
            return (symbol, left, right)

    if 'S' in table[0][n-1]:
        return build_tree(0, n-1, 'S')
    else:
        return None

# ---------------------------
# EXAMPLE USAGE
# ---------------------------
tokens = ['the', 'cat', 'chased', 'the', 'mouse']

# CYK grammar
grammar = {
    'S': [['NP', 'VP']],
    'VP': [['V', 'NP']],
    'NP': [['Det', 'N']],
    'Det': [['the']],
    'N': [['cat'], ['mouse']],
    'V': [['chased']]
}

# Run CYK
cyk_tree = cyk_parser(grammar, tokens)
if cyk_tree:
    print("CYK Parse Tree (tuple):")
    print(cyk_tree)
    nltk_tree = tuple_to_nltk_tree(cyk_tree)
    nltk_tree.pretty_print()
else:
    print("No parse tree found.Invalid input sentence")

# ---------------------------
# PROBABILISTIC CYK PARSER
# ---------------------------
def pcyk_parser(grammar, tokens):
    n = len(tokens)
    table = [[defaultdict(float) for _ in range(n)] for _ in range(n)]
    back = [[{} for _ in range(n)] for _ in range(n)]

    for i, token in enumerate(tokens):
        for lhs in grammar:
            for rhs, prob in grammar[lhs]:
                if len(rhs) == 1 and rhs[0] == token:
                    if prob > table[i][i][lhs]:
                        table[i][i][lhs] = prob
                        back[i][i][lhs] = token

    for l in range(2, n+1):
        for i in range(n - l + 1):
            j = i + l - 1
            for k in range(i, j):
                for A in grammar:
                    for (rhs, prob) in grammar[A]:
                        if len(rhs) == 2:
                            B, C = rhs
                            prob_B = table[i][k].get(B, 0)
                            prob_C = table[k+1][j].get(C, 0)
                            if prob_B and prob_C:
                                new_prob = prob * prob_B * prob_C
                                if new_prob > table[i][j].get(A, 0):
                                    table[i][j][A] = new_prob
                                    back[i][j][A] = (k, B, C)

    def build_tree(i, j, symbol):
        if i == j:
            return (symbol, back[i][j][symbol])
        k, B, C = back[i][j][symbol]
        return (symbol, build_tree(i, k, B), build_tree(k+1, j, C))

    if 'S' in table[0][n-1]:
        return build_tree(0, n-1, 'S')
    else:
        return None

# ---------------------------
# CONVERT TO nltk.Tree & DISPLAY
# ---------------------------
def tuple_to_nltk_tree(tree_tuple):
    if isinstance(tree_tuple, tuple):
        label = tree_tuple[0]
        children = [tuple_to_nltk_tree(child) for child in tree_tuple[1:]]
        return Tree(label, children)
    else:
        return tree_tuple

# PCYK grammar
p_grammar = {
    'S': [ (['NP', 'VP'], 1.0) ],
    'VP': [ (['V', 'NP'], 1.0) ],
    'NP': [ (['Det', 'N'], 1.0) ],
    'Det': [ (['the'], 1.0) ],
    'N': [ (['cat'], 0.5), (['mouse'], 0.5) ],
    'V': [ (['chased'], 1.0) ]
}

# Run PCYK
pcyk_tree = pcyk_parser(p_grammar, tokens)
if pcyk_tree:
    print("\nPCYK Parse Tree (tuple)- returns the most probable parse tree:")
    print(pcyk_tree)
    nltk_tree = tuple_to_nltk_tree(pcyk_tree)
    nltk_tree.pretty_print()
else:
    print("No parse tree found.Invalid input sentence")

"""## Machine Translation

Sep2Seq model (LSTM-LSTM)
"""

import torch
import torch.nn as nn
import torch.optim as optim
import random
import numpy as np

# Device config
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Example toy vocab
SRC_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, 'hello': 3, 'world': 4}
TRG_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, 'hola': 3, 'mundo': 4}
SRC_itos = {i: s for s, i in SRC_vocab.items()}
TRG_itos = {i: s for s, i in TRG_vocab.items()}

INPUT_DIM = len(SRC_vocab)
OUTPUT_DIM = len(TRG_vocab)
HID_DIM = 256
EMB_DIM = 128
N_EPOCHS = 100
LEARNING_RATE = 0.01

# --------------------------
# Encoder
# --------------------------
class Encoder(nn.Module):
    def __init__(self, input_dim, emb_dim, hid_dim):
        super().__init__()
        self.embedding = nn.Embedding(input_dim, emb_dim)
        self.lstm = nn.LSTM(emb_dim, hid_dim)     # IF U R USING RNN ---> self.rnn = nn.RNN(emb_dim, hid_dim)

    def forward(self, src):
        embedded = self.embedding(src)
        outputs, (hidden, cell) = self.lstm(embedded)
        return hidden, cell

        '''# RNN version
        outputs, hidden = self.rnn(embedded)
        return hidden, None'''

# --------------------------
# Decoder
# --------------------------
class Decoder(nn.Module):
    def __init__(self, output_dim, emb_dim, hid_dim):
        super().__init__()
        self.embedding = nn.Embedding(output_dim, emb_dim)
        self.lstm = nn.LSTM(emb_dim, hid_dim)     # IF U R USING RNN ----> self.rnn = nn.RNN(emb_dim, hid_dim)
        self.fc_out = nn.Linear(hid_dim, output_dim)

    def forward(self, input, hidden, cell):
        input = input.unsqueeze(0)
        embedded = self.embedding(input)
        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))
        prediction = self.fc_out(output.squeeze(0))
        return prediction, hidden, cell

        ''' # RNN version
        output, hidden = self.rnn(embedded, hidden)
        prediction = self.fc_out(output.squeeze(0))
        return prediction, hidden, None'''

# --------------------------
# Seq2Seq Model
# --------------------------
class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder

    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        batch_size = src.shape[1]
        trg_len = trg.shape[0]
        trg_vocab_size = self.decoder.embedding.num_embeddings

        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(device)
        hidden, cell = self.encoder(src)  # RNN version hidden = self.encoder(src)

        input = trg[0, :]  # <sos>
        for t in range(1, trg_len):
            output, hidden, cell = self.decoder(input, hidden, cell)    # RNN version output, hidden, _ = self.decoder(input, hidden, None)
            outputs[t] = output
            top1 = output.argmax(1)
            input = trg[t] if random.random() < teacher_forcing_ratio else top1

        return outputs

# --------------------------
# Data Example
# --------------------------
def sentence_to_tensor(vocab, sentence):
    tokens = [vocab[word] for word in sentence]
    tokens = [vocab['<sos>']] + tokens + [vocab['<eos>']]
    return torch.tensor(tokens, dtype=torch.long).unsqueeze(1).to(device)  # [len, 1]

example_src = ['hello', 'world']
example_trg = ['hola', 'mundo']

src_tensor = sentence_to_tensor(SRC_vocab, example_src)
trg_tensor = sentence_to_tensor(TRG_vocab, example_trg)

# --------------------------
# Train Model
# --------------------------
encoder = Encoder(INPUT_DIM, EMB_DIM, HID_DIM).to(device)
decoder = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM).to(device)
model = Seq2Seq(encoder, decoder).to(device)

optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
criterion = nn.CrossEntropyLoss(ignore_index=TRG_vocab['<pad>'])

for epoch in range(N_EPOCHS):
    model.train()
    optimizer.zero_grad()
    output = model(src_tensor, trg_tensor)  # output: [trg_len, 1, output_dim]
    output_dim = output.shape[-1]

    output = output[1:].view(-1, output_dim)
    trg = trg_tensor[1:].view(-1)

    loss = criterion(output, trg)
    loss.backward()
    optimizer.step()

    if epoch % 10 == 0:
        print(f"Epoch {epoch} Loss: {loss.item():.4f}")

# --------------------------
# Inference
# --------------------------
def translate_sentence(model, sentence, src_vocab, trg_vocab, trg_itos, max_len=10):
    model.eval()
    src_tensor = sentence_to_tensor(src_vocab, sentence)
    hidden, cell = model.encoder(src_tensor)

    input = torch.tensor([trg_vocab['<sos>']], device=device)

    translated_tokens = []

    for _ in range(max_len):
        output, hidden, cell = model.decoder(input, hidden, cell)
        top1 = output.argmax(1).item()
        if top1 == trg_vocab['<eos>']:
            break
        translated_tokens.append(trg_itos[top1])
        input = torch.tensor([top1], device=device)

    return translated_tokens

# Test translation
print("Translation:", translate_sentence(model, ['hello', 'world'], SRC_vocab, TRG_vocab, TRG_itos))

"""## Transformer

### Transformer - MT
"""

import pandas as pd
from datasets import Dataset
from transformers import MarianMTModel, MarianTokenizer, TrainingArguments, Trainer

# Example dataset for translation (English to French)
data = {'src': ["Hello, how are you?", "I am a student.", "It is a nice day."],
        'tgt': ["Bonjour, comment ça va?", "Je suis un étudiant.", "C'est une belle journée."]}

# Convert to DataFrame
df = pd.DataFrame(data)
dataset = Dataset.from_pandas(df)

# Load MarianMT tokenizer and model
model_name = "Helsinki-NLP/opus-mt-en-fr"  # English to French
tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name)

# Tokenize the data
def tokenize_function(examples):
    # Tokenize the source and target text
    input_encodings = tokenizer(examples['src'], padding="max_length", truncation=True, max_length=128)
    target_encodings = tokenizer(examples['tgt'], padding="max_length", truncation=True, max_length=128)

    # Add decoder_input_ids (shifted target tokens)
    input_encodings['labels'] = target_encodings['input_ids']

    # Return the encodings in a dictionary
    return input_encodings

dataset = dataset.map(tokenize_function, batched=True)

# Split the dataset into train and test
dataset = dataset.train_test_split(test_size=0.2)

# Training arguments
training_args = TrainingArguments(
    output_dir="./output",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    logging_dir='./logs',
    logging_steps=10,
    report_to="none"  # Disable W&B logging
)

# Trainer setup
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset['train'],
    eval_dataset=dataset['test'],
    tokenizer=tokenizer
)

# Fine-tune the model
trainer.train()

from transformers import MarianMTModel, MarianTokenizer

# Load the pre-trained model and tokenizer for English to French translation
model_name = "Helsinki-NLP/opus-mt-en-fr"
model = MarianMTModel.from_pretrained(model_name)
tokenizer = MarianTokenizer.from_pretrained(model_name)

# Function to perform translation
def translate(sentence):
    # Encode the sentence to tensor format
    inputs = tokenizer.encode("en_fr: " + sentence, return_tensors="pt", padding=True)

    # Generate translated output
    outputs = model.generate(inputs, max_length=50, num_beams=4, early_stopping=True)

    # Decode the output
    translated = tokenizer.decode(outputs[0], skip_special_tokens=True)

    return translated

# Test the function with a sentence
sentence = "I am a student"
translated_sentence = translate(sentence)

print(f"Original: {sentence}")
print(f"Translated: {translated_sentence}")

"""no fine tuning"""

from transformers import MarianMTModel, MarianTokenizer

def translate(text, src_lang="en", tgt_lang="fr"):
    model_name = f"Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}"
    tokenizer = MarianTokenizer.from_pretrained(model_name)
    model = MarianMTModel.from_pretrained(model_name)

    # Proper encoding as per Hugging Face v4+
    encoded = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    generated = model.generate(**encoded)
    return tokenizer.decode(generated[0], skip_special_tokens=True)

# Example
print("Translation:", translate("I love programming."))

source_input = input()
print("Translation:", translate(source_input))

"""### Transformer-Text summarization"""

import pandas as pd
from datasets import load_dataset
from transformers import BartForConditionalGeneration, BartTokenizer, TrainingArguments, Trainer

# Example dataset for summarization
data = {'text': ["The quick brown fox jumps over the lazy dog. It is a beautiful day today.",
                 "The weather is warm and sunny. People are enjoying the outdoors."],
        'summary': ["A fox jumps over a lazy dog on a beautiful day.",
                    "The weather is sunny, and people are enjoying the outdoors."]}

# Convert to DataFrame
df = pd.DataFrame(data)
dataset = Dataset.from_pandas(df)


# Load pre-trained model and tokenizer for summarization (BART model)
model_name = "sshleifer/tiny-bart"
tokenizer = BartTokenizer.from_pretrained(model_name)
model = BartForConditionalGeneration.from_pretrained(model_name)

# Tokenize the dataset
def tokenize_function(examples):
    # Tokenize input text (articles) and target text (summaries)
    input_encodings = tokenizer(examples['text'], padding="max_length", truncation=True, max_length=512)
    target_encodings = tokenizer(examples['summary'], padding="max_length", truncation=True, max_length=150)

    # Add labels for decoder
    input_encodings['labels'] = target_encodings['input_ids']

    return input_encodings

# Apply the tokenizer to the entire dataset
dataset = dataset.map(tokenize_function, batched=True)

# Split into train and test
dataset = dataset.train_test_split(test_size=0.1)

# Training arguments
training_args = TrainingArguments(
    output_dir="./output",
    num_train_epochs=2,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    logging_dir='./logs',
    logging_steps=10,
    report_to="none"
)

# Trainer setup
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset['train'],
    eval_dataset=dataset['test'],
    tokenizer=tokenizer
)

# Fine-tune the model
trainer.train()

# Summarize a text
model.eval()
text = "The quick brown fox jumps over the lazy dog. It is a beautiful day today. The weather is nice."
inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
summary_ids = model.generate(inputs['input_ids'], max_length=150, num_beams=4, early_stopping=True)

summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
print(f"Original: {text}")
print(f"Summary: {summary}")

"""no finetuning"""

from transformers import BartForConditionalGeneration, BartTokenizer

def summarize(text):
    tokenizer = BartTokenizer.from_pretrained("facebook/bart-large-cnn")
    model = BartForConditionalGeneration.from_pretrained("facebook/bart-large-cnn")

    inputs = tokenizer([text], return_tensors="pt", max_length=1024, truncation=True)
    summary_ids = model.generate(inputs["input_ids"], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# Example
text = '''Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.
          Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.
          ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine.The application of ML to business problems is known as predictive analytics.
          Statistics and mathematical optimization (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning.
          From a theoretical viewpoint, probably approximately correct learning provides a framework for describing machine learning.'''
print("Summary:", summarize(text))

"""### Transformer-Sentiment

Sentiment.csv

text,label

"I love this product",positive

"This is terrible",negative

"Absolutely fantastic experience",positive

"I would never buy this again",negative

"Great quality and fast shipping",positive

"The item broke after one use",negative

"Very satisfied with my purchase",positive

"Totally disappointed with the service",negative

"Exceeded my expectations!",positive

"Not worth the money at all",negative
"""

!pip install transformers datasets scikit-learn

import pandas as pd
from datasets import Dataset
from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, TrainingArguments, Trainer
from sklearn.preprocessing import LabelEncoder
import torch

# Load and encode dataset
df = pd.read_csv("sentiment.txt")
le = LabelEncoder()
df["label"] = le.fit_transform(df["label"])  # positive=1, negative=0

dataset = Dataset.from_pandas(df)

# Load tokenizer and tokenize dataset
tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")

def tokenize(batch):
    return tokenizer(batch["text"], padding=True, truncation=True)

dataset = dataset.map(tokenize, batched=True)
dataset = dataset.train_test_split(test_size=0.2)

# Load model
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english", num_labels=2)

# Define training args
training_args = TrainingArguments(
    output_dir="output",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    evaluation_strategy="epoch",
    save_strategy="no",
    logging_strategy="epoch",
    report_to="none"  # Disable WandB logging
)


# Define Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    tokenizer=tokenizer
)

# Fine-tune
trainer.train()

from transformers import TextClassificationPipeline

pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, top_k=None)

print(pipe("I love this!"))
print(pipe("This is bad."))

"""no finetuning"""

#Using distilbert-base-uncased-finetuned-sst-2-english
from transformers import pipeline

def analyze_sentiment(text):
    sentiment_model = pipeline(
        "sentiment-analysis",
        model="distilbert-base-uncased-finetuned-sst-2-english"
    )
    return sentiment_model(text)[0]

# Example
print("Sentiment:", analyze_sentiment("I absolutely love this course!"))

